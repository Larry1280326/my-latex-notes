\documentclass[11pt]{article}
\usepackage[margin=0.8in]{geometry}
\usepackage[]{amsfonts, amssymb, amsmath, float, hyperref,fancyheadings, graphicx, derivative}
\pagestyle{fancy}
\lhead{Design and Analysis of Algorithms}
\chead{Larry128}
\rhead{COMP3711}
\renewcommand{\footrulewidth}{0.4pt}
\setcounter{tocdepth}{3} % set TOC
\newcommand{\indep}{\perp \!\!\! \perp}

% codeblocks
\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=3
}
\lstset{style=mystyle}
\lstdefinestyle{customc}{
  language=C++,
  % Add other settings here
}

%common math symbols
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\ddx}{\dfrac{d}{dx}}


% asymptotic notations
\newcommand\BigO[1]{$O($#1$)$}
\newcommand\BigOmega[1]{$\Omega($#1$)$}
\newcommand\BigTheta[1]{$\Theta($#1$)$}

% pgfplots
\usepackage{pgfplots}
\pgfplotsset{compat=1.18, width=8cm}
\usepackage{geometry}
\geometry{
a4paper,
total={170mm,257mm},
left=20mm,
top=20mm,
}

\begin{document}
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
            
        \Huge
        \textbf{COMP3711}
            
        \vspace{0.5cm}
        \LARGE
        Design and Analysis of Algorithms
            
        \vspace{1.5cm}
            
        Larry128
            
        \vfill
            
        A summary notes for revision
            
        \vspace{0.8cm}
                
        \Large
        HKUST, Fall 2024-2025
            
    \end{center}
\end{titlepage}

\tableofcontents
\newpage


\section{Prerequisites}
\subsection{Input size of Problems}

\begin{tabular}{ll}
\textbf{Input size}& how large the input is.\\
\textbf{Assumption}& 1. any number can be stored in a computer word\\
&2. each arithmetic operation takes constant time\\
\textbf{Examples}& Sorting: Size of the list or array\\
& Graph problems: Numbers of vertices and edges\\
& Searching: Number of input keys
\end{tabular}

\subsection{Asymptotic Notation}

\begin{enumerate}
\item Running time/ Cost of algorithms\\
\begin{tabular}{l}
 i. a function of input size: $T(n)$\\
 ii. number of operations (e.g., comparisons between two numbers)\\
 iii. using \textbf{asymptotic notation}, which ignores constants and non-dominant growth terms
\end{tabular}

\item Intuitions\\
\begin{tikzpicture}[scale= 0.05]
\draw [-latex](-10, 0) -- (100, 0) node[right]{$n$};
\draw [-latex](0, -10) -- (0, 100) node[left]{$T(n)$};
\draw [red] plot [smooth, tension=1] coordinates {(20,20) (30,20) (40,25) (50,30)(60, 35) (70,40) (80, 50)(90, 60)} node[right]{Algorithm 1};
\draw [blue] plot [smooth, tension=1] coordinates {(20,10) (30,15) (40,35) (50,50)(60, 60) (70,70) (80, 80)(90, 90)} node[left]{Algorithm 2};
\end{tikzpicture}\\
From the figure above, Algorithm 1 is better for large $n$.\\

\item Rigorous definition of asymptotic notation\\
\begin{tabular}{|c|l|}
\hline
Upper bound $T(n) = O(f(n))$& if $\exists c > 0$ and $n_0 \geq 0$ such that $\forall n \neq n_0 , T(n) \leq c f(n)$\\
\hline
Lower bound $T(n) = \Omega(f(n))$& if $\exists c > 0$ and $n_0 \neq 0$ such that $\forall n \neq n_0 , T(n) \geq c f(n)$\\
\hline
Tight bound $T(n) = \Theta(f(n))$& if $T(n) = O(f(n))$ and $T(n) = \Omega(f(n))$\\
\hline
\end{tabular}

\item Big-O Notation\\
\begin{tikzpicture}[scale= 0.05]
\draw [-latex](-10, 0) -- (100, 0) node[right]{$n$};
\draw [-latex](0, -10) -- (0, 100) node[left]{$T(n)$};
\draw [red] (0, 0) -- (100, 100) node[left]{$c \cdot g(n)$};
\draw [blue] plot [smooth, tension = 1] coordinates{(0, 10) (20, 20) (30, 25) (40, 28) (50, 31) (60, 35) (70, 39) (80, 45)} node[right]{$T(n)$};
\draw [dotted] (20, 20) -- (20, 0) node[below]{$n_{0}$};
\end{tikzpicture}
$$T(n)= O(g(n)) \iff \exists n_{0} > 0 \text{ s.t. } \forall n \geq n_0, 0 \leq T(n) \leq c \cdot g(n)$$
Below are some examples of Big-O notation proofs
\begin{enumerate}
\item $T(n) = n, g(n) = n \log_{2} n$\\
We wish to proof $T(n) = n \in O(n \log_2 n)$.\\
Choose $c = 1, n_{0} = 2$, for all $n \geq 2 = n_0$, 
$$1 \leq \log_2 n \iff n \leq n \log_2 n \iff n \leq c \cdot n \log_2 n$$
\begin{center}
\begin{tikzpicture} [scale = 1]
\begin{axis}[clip = false,
axis lines= middle,
xmin = -10,
xmax = 100,
ymin= -10,
ymax = 100,
xlabel = {$n$},
ylabel = {$T(n)$}]
\addplot[domain = -10:100, color = red]{x}{node[right]{$T(n) = n$}};
\addplot[domain = 0:23, color = blue]{x*log2 x}{node[right]{$g(n) = n \log_2 n$}};
\end{axis}
\end{tikzpicture}
\end{center}
\item $T(n) = n^2, g(n) = n$\\
We wish to proof $T(n) = n^2 \not \in O(g(n))$ by contradiction.\\
Suppose there exists some $c$ and $n_0$ such that for all $n \geq n_0$, $n^2 \leq c \cdot n$. Then, $n \leq c$, $\forall n \geq n_0$, which is not possible as $c$ is a constant and $n$ can be arbitrarily large.
\begin{center}
\begin{tikzpicture} [scale = 1]
\begin{axis}[clip = false,
axis lines= middle,
xmin = -10,
xmax = 100,
ymin= -10,
ymax = 100,
xlabel = {$n$},
ylabel = {$T(n)$}]
\addplot[domain = -10:100, color = blue]{x}{node[right]{$g(n) = n$}};
\addplot[domain = 0:10, color = red]{x^2}{node[right]{$T(n) = n^2$}};
\end{axis}
\end{tikzpicture}
\end{center}
\end{enumerate}
\item Big-$\Omega$ Notation\\
\begin{tikzpicture}[scale= 0.05]
\draw [-latex](-10, 0) -- (100, 0) node[right]{$n$};
\draw [-latex](0, -10) -- (0, 100) node[left]{$T(n)$};
\draw [red] (0, 20) -- (100, 30) node[left]{$c \cdot g(n)$};
\draw [blue] plot [smooth, tension = 1] coordinates{(0, 10) (20, 20) (30, 25) (40, 28) (50, 31) (60, 35) (70, 39) (80, 45)} node[right]{$T(n)$};
\draw [dotted] (25, 23) -- (25, 0) node[below]{$n_{0}$};
\end{tikzpicture}
$$T(n)= \Omega(g(n)) \iff \exists n_{0} > 0 \text{ s.t. } \forall n \geq n_0, 0 \leq c \cdot g(n) \leq T(n)$$
\\
\item Big-$\Theta$ Notation\\
\begin{tikzpicture}[scale= 0.05]
\draw [-latex](-10, 0) -- (100, 0) node[right]{$n$};
\draw [-latex](0, -10) -- (0, 100) node[left]{$T(n)$};
\draw [red] (0, 20) -- (100, 30) node[left]{$c \cdot g(n)$};
\draw [blue] plot [smooth, tension = 1] coordinates{(0, 10) (20, 20) (30, 25) (40, 28) (50, 31) (60, 35) (70, 39) (80, 45)} node[right]{$T(n)$};
\draw [green] (0, 0) -- (100, 100) node[left]{$c' \cdot g(n)$};
\draw [dotted] (25, 23) -- (25, 0) node[below]{$n_{0}$};
\end{tikzpicture}
$$T(n) = \Theta(f(n)) \iff T(n) = O(f(n)) \text{ and }T(n) = \Omega(f(n))$$

\item Implementation and experimentation are needed sometimes\\
If algorithm A is $T_1 (n) = 10n \in \Theta(n)$, algorithm B is $T_2 (n) = 1000n \in \Theta(n)$, but algorithm A is superior in practice. In this case, Implementation and experimentation are needed.

\item Basic facts on exponents and logarithms
\begin{enumerate}
\item $2^{2n} \neq \Theta (2^n)$, proof: set $x=2^n$, then $x^2 \neq \Theta (x)$
\item $2^{n+2} = 4 \cdot 2^n = \Theta (2^n)$
\item $\log_a(n^b) = \dfrac{b \log n}{\log a} = \Theta (\log n)$
\item $\log_{b} a = \dfrac{1}{\log_{a} b}$
\item $a^{\log_{b} n} = n^{\log_{b} a}$
\end{enumerate}

\item Important note on growth of functions
$$k < \log n < n^{a} < n \log n < n^{b} < c^{n}$$
,where $k, c \in \R , 0 < a < 2, b \geq 2 \text{ are constants}$
\begin{enumerate}
\item $999^{999^{999}} = \Theta (1)$
\item $\log \log n = O(\log n)$, proof: for $n \geq 2$, $\log \log n \leq \log n$\\
\begin{tikzpicture} [scale = 1]
\begin{axis}[clip = false,
axis lines= middle,
xmin = -10,
xmax = 100,
ymin= -2,
ymax = 6,
xlabel = {$n$},
ylabel = {$T(n)$}]
\addplot[domain = 0:100, color = red]{log10 x}{node[right]{$\log x$}};
\addplot[domain = 0:100, color = blue]{log10 log10 x}{node[right]{$\log \log x$}};
\end{axis}
\end{tikzpicture}
\item $n \log n = O(\dfrac{n^2}{\log n})$\\
proof: To show $n \log n = O(\dfrac{n^2}{\log n})$, it suffices to show that there exists a $C > 0$, such that $n \log n < C \cdot \dfrac{n^2}{\log n}$ for sufficiently large $n$.
\begin{align*}
& n \log n  < C \cdot \dfrac{n^2}{\log n}\\
\iff & (\log n)^2 < C \cdot n
\end{align*}
It's obvious that for large $n$, $\log (n) < n^{\epsilon}$ for $\epsilon > 0$, then we can pick $\epsilon =  \dfrac{1}{2}$
\begin{align*}
\log n < n^{\frac{1}{2}}\\
(\log n) ^2 < n
\end{align*}
Since $C > 0$, we can see $(\log n) ^2 < n < C \cdot n$. We are done.
\end{enumerate}

\item Extra Examples
\begin{enumerate}
\item $1000n + n \log n = O(n \log n)$
\item $n^2 + n \log (n^3) = n^2 + 3n \log n = O(n^2)$
\item $n ^3 = \Omega (n)$
\item $n^3 = O(n ^{10})$
\item Let $f(n)$ and $g(n)$ be non-negative functions. Using basic definition of $\Theta$-notation, proof that $\max\{f(n), g(n)\} = \Theta(f(n) + g(n))$
\begin{enumerate}
\item Step 1: proof $\max \{ f(n), g(n)\} = O(f(n) + g(n))$\\
For all $n$, $\max \{ f(n), g(n)\}$ is either equal to $f(n)$ or equal to $g(n)$. So we can deduce that $\max \{ f(n), g(n)\} \leq f(n) + g(n)$. Therefore, $\max \{ f(n), g(n)\} = O(f(n) + g(n))$.
\item Step 2: proof $\max \{ f(n), g(n)\} = \Omega(f(n) + g(n))$\\
Note that $\max \{ f(n), g(n)\} \geq f(n)$ and $\max \{ f(n), g(n)\} \geq g(n)$. So 
\begin{align*}
\max \{ f(n), g(n)\} + \max \{ f(n), g(n)\} \geq f(n) + g(n) \\
2\cdot \max \{ f(n), g(n)\} \geq f(n) + g(n)\\
\max \{ f(n), g(n)\} \geq \frac{1}{2} (f(n) + g(n))
\end{align*}
Then, we have $\max \{ f(n), g(n)\} = \Omega (f(n) + g(n))$
\end{enumerate}
\item if $A = \log \sqrt{n}, B = \sqrt{\log n}$, then $A = \Omega(B)$\\
proof: $A = \log \sqrt{n} = \dfrac{1}{2} \log n = \Theta(\log n)$, $B = \sqrt{\log n} = \Theta(\log n)$. We can simply deduce that $\log \sqrt{n} = \Omega(\sqrt{\log n})$
\item Bounds of series - Arithmetic Series\\
Proof that $\sum_{i=1}^{n} i = 1 +2 +3 + \cdots + (n-1) + n = \Theta (n^2)$
\begin{enumerate}
\item Approach 1: use formula $\sum_{i=1}^{n} i = \dfrac{n(1+n)}{2} = \Theta (n^2)$
\item Approach 2
\begin{enumerate}
\item Step 1: proof $\sum_{i=1}^{n} i = O(n^2)$
\begin{align*}
\sum_{i=1}^{n} i &= 1 +2 +3 + \cdots + (n-1) + n\\
& \leq n + n + ... + n\\
& = \sum_{i=1}^{n} n\\
& = n \cdot n \\
& = n^2 =O(n^{n})
\end{align*}
\item Step 2: proof $\sum_{i=1}^{n} i = \Omega(n^2)$
\begin{align*}
\sum_{i=1}^{n} i &= 1 +2 +3 + \cdots + (n-1) + n\\
& \geq 0+ 0 + \cdots + 0 +...+ \frac{n}{2} + (\frac{n}{2} +1) + \cdots +n \\
& \geq \frac{n}{2} \cdot \frac{n}{2} \\
& = \frac{n^2}{4} = \Omega(n^2)
\end{align*}
\end{enumerate}
Then, we can say that $\sum_{i=1}^{n} i = \Theta (n^2)$
\end{enumerate}
\item Bounds of series - Polynomial Series\\
Proof that $\sum_{i=1}^{n} i^{c} = 1^c +2^c +3^c + \cdots + (n-1)^c + n^c = \Theta (n^{c+1})$\\
(The proof is more or less the same as the approach 2 of arithmetic series.)
\item Bounds of series - Harmonic Series $H_n$\\
Proof that $H_{n} = \sum_{i=1}^{n} \frac{1}{i} = \Theta(\log n)$.\\
Let $k = \log_2 n$, then $n = 2^k$. \\
\begin{tabular}{|l|l|l|l|}
\hline
index&lower bound& parts of $H_n$& upper bound\\
\hline
0&$\dfrac{1}{2}$& $1$& $1$\\
\hline
1&$2 \times \dfrac{1}{4} = \dfrac{1}{2}$ &$\dfrac{1}{2}+ \dfrac{1}{3}$& $2 \times \dfrac{1}{2} = 1$\\
\hline
2&$4 \times \dfrac{1}{8} = \dfrac{1}{2}$ &$\dfrac{1}{4}+ \dfrac{1}{5}+ \dfrac{1}{6}+ \dfrac{1}{7}$& $4 \times \dfrac{1}{4} = 1$\\
\hline
&$\cdots$ & $\cdots$ & $\cdots$\\
\hline
k-1&$2^{k-1} \times \dfrac{1}{2^k} = \dfrac{1}{2}$ &$\dfrac{1}{2^{k-1}}+\dfrac{1}{2^{k-1}+1} \cdots +\dfrac{1}{2^{k} -1}$& $2^{k-1} \times \dfrac{1}{2^{k-1}} = 1$\\
\hline
k&$0$& $\dfrac{1}{2^k} = \dfrac{1}{n}$& 1\\
\hline
\end{tabular}\\\\
Therefore, $H_n < \sum_{i=0}^{k} 1 = k +1 = \log_2 n +1 = O(\log n)$ and $H_n > \sum_{i=0}^{k-1} \dfrac{1}{2} +0 = \dfrac{k}{2} = \frac{\log_2 n}{2} = \Omega (\log n)$. So, $H_{n} = \sum_{i=1}^{n} \frac{1}{i} = \Theta(\log n)$.
\end{enumerate}
\item Past exam questions\\
We have two algorithms, A and B. Let $T_A (n)$ and $T_B (n)$ denote the time complexities of algorithm A and B respectively, with respect to the input size n. 
\begin{enumerate}
\item $T_{A} (n) = \Theta (n^{1.5}), T_{B} (n) = \Theta (\frac{n^{2}}{(\log n)^3})$\\
Note that there must exist $n_{0}$ such that for all $n \geq n_{0}$, \begin{align*}
(\log n)^3 \leq n^{1/2} \iff n^{1.5} \leq \frac{n^2}{(\log n)^3}
\end{align*}
We can conclude that algorithm A is faster.
\item $T_{A} (n) = O (n^{2}), T_{B} (n) = \Omega (2^{\sqrt{n}})$\\ Obviously algorithm A is faster since A is polynomial while B is exponential.
\item $T_{A} (n) = O (\log n), T_{B} (n) = \Theta (2^{\log_2 \log_2 n})$\\ Note that $2^{\log_2 \log_2 n} = \log_2 n = \Theta
(\log n)$, so we don't have enough information to justify.
\item $T_{A} (n) = \Theta ((\log n)^3), T_{B} (n) = \Theta (\sqrt[3]{n})$\\ Obviously algorithm A is faster since A is logarithmic while B is polynomial.
\item $T_{A} (n) =O(n^4), T_{B} (n) = O(n^3)$\\
Since both are upper bounds, we cannot conclude anything.
\item $T_{A} (n) =\Omega(n^3), T_{B} (n) = O(n^{2.8})$\\
B is faster since the lower bound of A is greater than the upper bound of B.
\item $T_{A} (n) =\Theta(n^3), T_{B} (n) = \Theta(4^{\log_5 n})$\\ Consider $4^{\log_5 n} = n^{\log_5 4} = \Theta(n)$, we cannot conclude anything from that.
\item (Stirling's formula) Proof that $\log(n!) = \Theta (n \log n)$.\\
First we proof that $\log (n!) = O(n \log n)$.
\begin{align*}
\log (n!) &= \log (n (n-1) \cdots 2 \cdot 1)\\ 
&= \log n + \log (n-1) + \cdots + \log 1 \\
&\leq \log n + \log n + \cdots + \log n \\
&= n \log n = O(n \log n)
\end{align*}
Then we proof that $\log (n!) = \Omega (n \log n)$.
\begin{align*}
\log (n!) &= \log (n (n-1) \cdots 2 \cdot 1)\\ 
&= \log n + \log (n-1) + \cdots + \log 1 \\
&\geq \log n + \log (n-1) + \cdots + \log (\frac{n}{2}) \\
&\geq \log \frac{n}{2} + \log \frac{n}{2} + \cdots + \log \frac{n}{2}\\
&= \frac{n}{2} \log \frac{n}{2}\\
&= \frac{n}{2} (\log n -\log 2) = \Omega(n \log n)
\end{align*}
Finally, we can conclude that $\log (n!) = \Theta (n \log n)$
\end{enumerate}
\end{enumerate}

\subsection{Introduction to Algorithms}
\begin{enumerate}
\item What is an algorithm?\\
An algorithm is an explicit, precise, unambiguous, mechanically-executable sequence of elementary instructions.
\item Examples of algorithms
\begin{enumerate}
\item Adding two numbers\\
Input: 2 numbers $x = \overline{x_{n} x_{n-1} \cdots x_{1}}, y = \overline{y_{n} y_{n-1} \cdots y_{1}}$.\\
Output: A number $z= \overline{z_{n+1} z_{n} \cdots z_{1}}$, such that $z = x+y$.
\begin{lstlisting}[language = C++]
/*We assume x, y are arrays of length n, z is of length n+1 */
int c = 0; // offset
for (int i = 0; i < n; ++i){
	z[i] = x[i] + y[i] + c;
	if (z[i] >= 10) {
		c = 1;
		z[i] = z[i] - 10;	
	}else c = 0;
}
z[n] = c;
\end{lstlisting}
\item Sorting Problem\\
Input: An array $A[1 \cdots n]$ of elements, e.g., $[4, 8, 2, 7, 5, 6, 9, 3]$\\
Output: An array $A[1 \cdots n]$ of elements in sorted order (ascending), e.g., $[2, 3, 4, 5, 6, 7, 8, 9]$
\begin{enumerate}
\item Selection sort
\begin{lstlisting}[language = C++]
/* Selection sort for ascending order */
for (int i=0; i<n-1; ++i){ 
	// in the i-th pass, find the smallest element in A[i, i+2, ..., n] and swap it with A[i]
	for (int j=i+1; j<n; ++j){
		if (A[i] > A[j]){ // swap A[i] and A[j] if A[i] > A[j]
			int temp = A[i];
			A[i] = A[j];
			A[j] = temp;	
		}	
	}
}
\end{lstlisting}
For example:\\
\begin{tabular}{|l|l|}
\hline
i = 0& (5, 2, 8, 6, 7, 1) $\rightarrow$(2, 5, 8, 6, 7, 1) $\rightarrow$ (1, 5, 8, 6, 7, 2)\\
\hline
i = 1& (1, 5, 8, 6, 7, 2) $\rightarrow$ (1, 2, 8, 6, 7, 5)\\
\hline
i = 2& (1, 2, 8, 6, 7, 5) $\rightarrow$ (1, 2, 6, 8, 7, 5) $\rightarrow$ (1, 2, 5, 8, 7, 6)\\
\hline
i = 3& (1, 2, 5, 8, 7, 6) $\rightarrow$ (1, 2, 5, 7, 8, 6) $\rightarrow$ (1, 2, 5, 6, 8, 7)\\
\hline
i = 4& (1, 2, 5, 6, 8, 7)\\
\hline
\end{tabular}\\\\
\textbf{Running time of selection sort}\\
For selection sort, the total cost of algorithm (total number of comparisons) can be given by $$(n-1) + (n-2) + \cdots +2+1 = \sum_{i = 1}^{n-1} i = \dfrac{(n-1)(1+n-1)}{2} = \dfrac{n(n-1)}{2} = \Theta(n^2)$$
Alternatively, we could think in this way: note that the algorithm runs through all possible $(i, j)$ pairs  with $1 \leq i \leq j \leq n$. There are ${n \choose 2} = \dfrac{n (n-1)}{2}$ possible pairs. So, that's the cost of selection sort.\\
Note: The cost is \textit{always} the same for any array of size $n$.\\\\
\textbf{Proof of correctness of selection sort}\\
\textit{Claim}: When selection sort terminates, the array is sorted.\\
\textit{Proof}: By induction on $n$.\\
When $n = 1$, the algorithm is obviously correct because there's only one element in the array.\\
Assume that the algorithm sorts every array of size $n - 1$ correctly.\\
Now, consider what the algorithm does on $A[1 \cdots n]$.
\begin{enumerate}
\item It first puts the smallest item in $A[1]$.
\item It then runs the selection sort on $A[2 \cdots n]$ of size $n$. By inductive assumption, this sorts the items in $A \cdots n$.
\item Since $A[1]$ is smaller than every item in $A[2 \cdots n]$, all the items in $A[1 \cdots n]$ are now sorted.
\end{enumerate}

\item Insertion sort
\begin{lstlisting} [language = C++]
/* Insertion sort for ascending order */
for (int i=1; i<n; ++i){
	int j= i-1;
	while (j>=0 && A[j]>A[j+1]){
		int temp = A[j];
		A[j] = A[j+1];
		A[j+1] = temp;
		j= j -1;	
	}
}
\end{lstlisting}
For example:\\
\begin{tabular}{|l|l|}
\hline
i = 1& ( 5 1 8 6 3 2 )$\rightarrow$( 1 5 8 6 3 2 )$\rightarrow$( 1 5 8 6 3 2 )\\
\hline
i = 2& ( 1 5 8 6 3 2 )$\rightarrow$( 1 5 8 6 3 2 )\\
\hline
i = 3& ( 1 5 8 6 3 2 )$\rightarrow$( 1 5 6 8 3 2 )$\rightarrow$( 1 5 6 8 3 2 )\\
\hline
i = 4& ( 1 5 6 8 3 2 )$\rightarrow$( 1 5 6 3 8 2 )$\rightarrow$( 1 5 3 6 8 2 )$\rightarrow$( 1 3 5 6 8 2 )$\rightarrow$( 1 3 5 6 8 2 )\\
\hline
i = 5& ( 1 3 5 6 8 2 )$\rightarrow$( 1 3 5 6 2 8 )$\rightarrow$( 1 3 5 2 6 8 )$\rightarrow$( 1 3 2 5 6 8 )$\rightarrow$( 1 2 3 5 6 8 )\\
&$\rightarrow$( 1 2 3 5 6 8 )\\
\hline
\end{tabular}\\\\
\textbf{Running time of insertion sort}\\
Total cost of insertion sort/ number of comparison is \textit{at most} $$\sum_{i=2}^{n} (i-1) = \dfrac{(n)(n-1)}{2} = \Theta (n^2)$$
. This worst case happens when the input array in descending order.\\
Note: unlike selection sort which always uses $\dfrac{n(n-1)}{2}$ comparisons for each array of size $n$, the number of comparisons (running time) of Insertion Sort depends on the input array, and ranges between $n-1$ and $\dfrac{n(n-1)}{2}$.\\$n-1$ when the input array is originally sorted.\\
\textbf{Proof of correctness of insertion sort}\\
\begin{tabular}{|p{4cm}|c|p{6cm}|}
\hline
$A[1 \cdots i-1]$-sorted&key& $A[i+1 \cdots n]$-unsorted\\
\hline
\end{tabular}\\
After step $i$, items in $A[1 \cdots i]$ are in proper order. The $i$-th iteration puts key $A[i]$ in proper place.

\item Wild-Guess sort\\
First, we create an array with random permutation, $\vec{\pi}= [4, 7, 1, 3, 8, ...]$, of length $n$.
\begin{lstlisting}[language=C++]
/* check if the order is correct or not */
bool check(const int A[], const int& n){
	for (int i=0; i<n-1; ++i){
		if ( A[pi[i]] > A[pi[i+1]] ) return false;
	}
	return true;
}
\end{lstlisting}
\begin{lstlisting}[language=C++]
if (check(A, n)) return;
else insertion_sort(A, n);
\end{lstlisting}
It has a very small probability that wild-guess sort is faster than insertion sort but most likely it's slower.
\end{enumerate}
\end{enumerate}
\end{enumerate}

\subsection{Algorithm Evaluation}
\begin{enumerate}
\item Measure Criteria
\begin{enumerate}
\item Memory (space complexity)
\item Running time (time complexity) \textit{(We use this.)}
\end{enumerate}
\item Methods to measure
\begin{enumerate}
\item Empirical: depends on actual implementation, hardware
\item Analytical: depends only on the algorithms \textit{(We use this.)}
\end{enumerate}
\item Analysis of Algorithm\\
To illustrate them, we use \textbf{insertion sort} as an example.
\begin{enumerate}
\item Best-Case Analysis\\
If the input array is sorted originally, then the running time is just $T(n) = n -1 = \Theta (n)$. We call this "Best-Case Analysis".
\item Worst-Case Analysis \textit{(Commonly used)}\\
If the input array is inversely sorted, then the running time is $T(n) = \dfrac{n(n-1)}{2} = \Theta (n)$. We call this "Worse-Case Analysis".
\item Average-Case Analysis\\
We assume each of the $n!$ permutations of the $n$ numbers is equally likely, then intuitively (but not rigorously) $T(n) = \sum_{i=2}^{n} \dfrac{i-1}{2} = \dfrac{n (n-1)}{4} = \Theta (n^2)$. We call this "Average-Case Analysis".
\end{enumerate}
\end{enumerate}

\section{Divide and Conquer}
\subsection{Basic Ideas with Examples}
Main idea of \textbf{Divide and Conquer} is that we solve a problem of size $n$ by breaking it into one or more smaller problems of size less than $n$. Then, we solve the smaller problems \textit{recursively} and combine their solutions to solve the original large problem. Here are some examples.
\begin{enumerate}
\item Binary Search\\
Input: a sorted (ascending/ descending) array $A[1 \cdots n]$ and an element $x$\\
Output: Return the index (position) of $x$, if $x$ is in $A$; otherwise return \textit{nil}.\\
The algorithm:
\begin{lstlisting}[language = C++]
int BinarySearch(int A[], int p, int r, int x){
	if (p > r) return -1;
	int q = (p + r)/2;
	if (A[q] = x) return q;
	if (x < A[q]) BinarySearch(A, p, q-1, x);
	else BinarySearch(A, p+1, r, x);
}
\end{lstlisting}
Then, we can call the function in this way:
\begin{lstlisting}[language = C++]
int i = BinarySearch(A, 0, sizeof(A)/sizeof(int), x);
\end{lstlisting}
Analysis of the algorithm:\\
Let $T(n)$ be the number of comparisons needed for an array with $n$ elements. $$T(n) = \begin{cases} 
T(n/2) +2& \text{if } n > 1\\
1& \text{if } n =1
\end{cases}$$Then  $T(n/2) $ since the size of input is reduced by half. $+ 2$ is for the comparisons in lines $4$ and $5$. The comparison in line $2$ is omitted as we assume $x$ is always in $A$. We can solve the recurrence relation by the \textit{expansion method}.
\begin{align*}
T(n) &= T(n/2) + 2\\
&= T(n/2^2) + 2 \cdot 2\\
&= T(n/2^3) + 3 \cdot 2\\
&=\cdots\\
&= T(n/2^i) + 2i\\
&=\cdots& \frac{n}{2^{i}} = 1 \iff i = \log_2 n\\
&= T(n/2^{\log_2 n}) + 2\log_2 n\\
&= T(1) + 2\log_2 n\\
&= 1+ 2\log_2 n = \Theta (\log n)
\end{align*}

\item Rotated Sorted Array\\
Let $A[1 \cdots n]$ be a sorted array of $n$ distinct numbers that has been rotated $n-k$ steps for some unknown integer $k \in [1, n-1]$. That is, $A[1 \cdots k]$ is sorted in increasing order, and $A[k+1 \cdots n]$ is also sorted in increasing order, and $A[n] < A[1]$. The following array $A$ is an example of $n=16$ elements with $k=10$. $$A=[9, 13, 16, 18, 19, 23, 28, 31, 37, \mathbf{42}, 0, 1, 2, 5, 7, 8]$$ We can design an $O(\log n)$-time algorithm to find the value of $k$.
\begin{lstlisting} [language = C++]
int findk(int A[], int p, int q){
	int m = (p+q)/2;
	if (A[m] > A[m+1]) return m; //base case: found the value
	if (A[m] >= A[1]) return findk(A, m+1, q); //search on the right hand side
	return findk(A, p, m-1);  //search on the left hand side
}
\end{lstlisting}
Then, we can call the algorithm like this
\begin{lstlisting} [language = C++]
int k = findk(A, 1, sizeof(A)/size(int));
\end{lstlisting}
Analysis of the algorithm:\\
It's similar to binary search $$T(n) = T(n/2) + c \implies T(n) = O(\log n)$$

\item Rotated Sorted Array (continued)\\
We can also design an $O(\log n)$-time algorithm that for any given $x$, find $x$ in the rotated sorted array, or report that it does not exist.
\begin{lstlisting} [language=C++]
int findx(int A[], int x){
	int k = findk(A, 1, n);
	if (x >= A[1]) return BinarySearch(A, 1, k, x); // search in  A[1...k]
	else return BinarySearch(A, k+1, n, x); // search in A[k+1 ... n]
}
\end{lstlisting}
Analysis of the algorithm:\\
This algorithm consist of one comparison, one \texttt{findk}, and \texttt{BinarySearch}. Therefore, $T(n) = O(\log n)$.

\item Finding the last 0\\
We are given an array $A[1 \cdots n]$ that contains a sequences of $0$ followed by a sequence of $1$ (e.g., $0001111111$). A contains at least one $0$ and one $1$.
\begin{enumerate}
\item Design an $O(\log n)$-time algorithm that finds the position $k$ of the last $0$, i.e., $A[k] = 0$ and $A[k+1] =1$.
\begin{lstlisting} [language = C++]
int findk(int A[], int p, int r){
	int mid = (p+r)/2;
	if (A[mid] ==0 && A[mid +1] == 1) return mid;
	if (A[mid] == 0) findk(A, mid + 1, r); // search on the right hand side
	else findk(A, p, mid); // search on the left hand side
}
\end{lstlisting}
\item Suppose that $k$ is much smaller than $n$. Design an $O(\log k)$-time algorithm that finds the position $O$ of the last $0$. (Hint: re-use solution of part (a).)
\begin{lstlisting}[language = C++]
i = 1;
while (A[i] == 0) {
	i = min(2*i, n);
}
findk(A, i/2, i);
\end{lstlisting}
The while loop will stop when it finds a $1$. Since each time we double the value of $i$, the while loop performs $2^i = k \implies i = \log_{2} k$ iterations. The first $1$ occurs somewhere between the positions $A[i/2 +1]$ and $A[i]$. To find it, we can call \texttt{findk(A, i/2, i)}, which has cost $\log (k/2) = O(\log k)$. Therefore, the total cost is $O(\log k)$.
\end{enumerate}
\end{enumerate}
\end{document}